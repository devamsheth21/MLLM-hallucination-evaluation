{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56829b48-6ef9-4ec2-ac01-5d74e4b06208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4dc31bd64a4e0b91752b51516dd87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-7b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-7b\",\n",
    "    cross_attn_every_n_layers=4\n",
    ")\n",
    "\n",
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce46b763-a226-4df4-aa78-006e1f210c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>this is an image of two cats sleeping on a couch<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "query_image = Image.open(\n",
    "    \"sample.jpg\"\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>this is an image of\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6dabc97-904b-4d9d-8c99-b01a5437707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "Flamingo model initialized with 1046992944 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: <image>this is an image of an image of an image of an image of an image of an image of an image of an image\n"
     ]
    }
   ],
   "source": [
    "## pipeline function --\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "def generate_response(image_path, input_text):\n",
    "    # Step 1: Load images\n",
    "    query_image = Image.open(image_path)\n",
    "\n",
    "    # Step 2: Preprocessing images\n",
    "    vision_x = [image_processor(query_image).unsqueeze(0)]\n",
    "    vision_x = torch.cat(vision_x, dim=0)\n",
    "    vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "    # Step 3: Preprocessing text\n",
    "    tokenizer.padding_side = \"left\"  # For generation, padding tokens should be on the left\n",
    "    lang_x = tokenizer([f\"{input_text}\"], return_tensors=\"pt\")\n",
    "\n",
    "    # Step 4: Generate text\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(generated_text[0])\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"sample.jpg\"\n",
    "input_text = \"<image>this is an image of\"\n",
    "\n",
    "# Generate response based on the image and text pair\n",
    "generated_response = generate_response(image_path, input_text)\n",
    "print(\"Generated response:\", generated_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
